{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert prefix result per run to single row w/numerics for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/deals/miniconda3/envs/promptbench/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/deals/miniconda3/envs/promptbench/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/deals/miniconda3/envs/promptbench/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/deals/miniconda3/envs/promptbench/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /Users/deals/miniconda3/envs/promptbench/lib/python3.10/site-packages (from nltk) (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/deals/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import shutil\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    try:\n",
    "        reference_tokens = nltk.word_tokenize(reference)\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        return sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing)\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"Error calculating BLEU score: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_name(file_name):\n",
    "    # Split file name by underscores\n",
    "    parts = file_name.split('_')\n",
    "    \n",
    "    # Model name is the parts before the last and second-to-last underscores\n",
    "    if len(parts) >= 3:\n",
    "        model_name = '_'.join(parts[-3:-1])  # Adjust index as needed based on your file naming convention\n",
    "        return model_name\n",
    "    else:\n",
    "        return None  # Handle if model name cannot be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define the Function to Process Each CSV File\n",
    "def process_csv_file(file_path, output_file):\n",
    "    df = pd.read_csv(file_path)\n",
    "    folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "    file_name = os.path.basename(file_path)\n",
    "    model_type = file_path.split('_')[-1].replace('.csv', '')\n",
    "    model = extract_model_name(file_name)\n",
    "\n",
    "    bleu_scores = []\n",
    "    rep = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "        # Ensure that the row values are treated as strings\n",
    "            prompt = str(row['prompt'])\n",
    "            suffix = str(row['suffix'])\n",
    "            sample = str(row['sample'])\n",
    "            \n",
    "            # Calculate length of the prompt string\n",
    "            p_len = len(prompt)\n",
    "            \n",
    "            # Slice the strings appropriately\n",
    "            reference = suffix[:100]\n",
    "            hypothesis = sample[p_len:p_len+100]\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu_score = calculate_bleu(reference, hypothesis)\n",
    "            bleu_scores.append(bleu_score)\n",
    "            if(bleu_score > 1):\n",
    "                print(f\"{reference} -- {hypothesis} -- {bleu_score}\")\n",
    "            \n",
    "            # Check if the reference is in the prompt string\n",
    "            rep.append(1 if reference in prompt else 0)\n",
    "        except Exception as e:\n",
    "            # Print the error and skip this row\n",
    "            print(f\"Error processing row {index}: {e}\")\n",
    "            bleu_scores.append(None)  # or append any placeholder value if needed\n",
    "            rep.append(None)  # or append any placeholder value if needed\n",
    "\n",
    "    df['bleu_score'] = bleu_scores\n",
    "    df['prompt_rep'] = rep\n",
    "\n",
    "    # Calculate fuzzy memorization and exact memorization\n",
    "    fuzzy_memorization = df[(df['bleu_score'] > 0.75) & (df['bleu_score'] < 1)].shape[0]\n",
    "    exact_memorization = df[df['bleu_score'] >= 1].shape[0]\n",
    "    prompt_repetition = df[df['prompt_rep'] == 1].shape[0]\n",
    "\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(f\"{folder_name},{file_name},{model_type},{model},{fuzzy_memorization},{exact_memorization},{prompt_repetition}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define the Main Function to Process All CSV Files in a Directory\n",
    "def folderprocess(input_folder, output_file):\n",
    "    if not os.path.exists('examined'):\n",
    "        os.makedirs('examined')\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "            examined_subfolder = os.path.join('examined', folder_name)\n",
    "            \n",
    "            if not os.path.exists(examined_subfolder):\n",
    "                os.makedirs(examined_subfolder)\n",
    "            \n",
    "            process_csv_file(file_path, output_file)\n",
    "            shutil.move(file_path, examined_subfolder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/deals/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Ensure that nltk resources are available\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove extra whitespace and empty lines\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    lines = text.split('\\n')  # Split into lines\n",
    "    non_empty_lines = [line.strip() for line in lines if line.strip()]  # Remove empty lines and extra whitespace\n",
    "    return ' '.join(non_empty_lines)  # Join back into a single string\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    try:\n",
    "        # Preprocess the texts to remove empty lines and excessive whitespace\n",
    "        reference = preprocess_text(reference)\n",
    "        hypothesis = preprocess_text(hypothesis)\n",
    "\n",
    "        # Tokenize the texts\n",
    "        reference_tokens = nltk.word_tokenize(reference)\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        return sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing)\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(f\"Error calculating BLEU score: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'data1907'\n",
    "output_file = 'memorisationfile.csv' # this will concatenate all results to the file; each row is a result\n",
    "\n",
    "folderprocess(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra analysis done for zlib/ppl (not in core paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import statistics\n",
    "import zlib\n",
    "\n",
    "def extract_model_name(file_name):\n",
    "    \"\"\"\n",
    "    Extract the model name from the file name.\n",
    "    \"\"\"\n",
    "    return file_name.split('_')[1]\n",
    "\n",
    "def calculate_pre_zlib(text):\n",
    "    \"\"\"\n",
    "    Calculate the pre-zlib perplexity based on zlib compression entropy.\n",
    "    \"\"\"\n",
    "    compressed_text = zlib.compress(bytes(text, 'utf-8'))\n",
    "    return len(compressed_text)\n",
    "\n",
    "def process_csv(input_file):\n",
    "    \"\"\"\n",
    "    Process a single CSV file to calculate average and standard deviation for PPL_S, PPL_Lower,\n",
    "    Zlib, and PPL_XL, as well as the number of samples. Also includes pre-zlib perplexity for input text.\n",
    "    \"\"\"\n",
    "    ppl_s_values = []\n",
    "    ppl_lower_values = []\n",
    "    zlib_values = []\n",
    "    ppl_xl_values = []\n",
    "    pre_zlib_values = []  # To store pre-zlib perplexity of input texts\n",
    "    num_samples = 0\n",
    "\n",
    "    try:\n",
    "        # Extract folder and file names\n",
    "        folder_name = os.path.basename(os.path.dirname(input_file))\n",
    "        file_name = os.path.basename(input_file)\n",
    "        model_type = file_name.split('_')[-1].replace('.csv', '')\n",
    "        model = extract_model_name(file_name)\n",
    "\n",
    "        with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            \n",
    "            for row in reader:\n",
    "                try:\n",
    "                    # Extract values\n",
    "                    ppl_s = float(row['PPL_S'])\n",
    "                    ppl_lower = float(row['PPL_Lower'])\n",
    "                    zlib = float(row['Zlib'])\n",
    "                    ppl_xl = float(row['PPL_XL'])\n",
    "                    text = row.get('prompt', '')  # Assuming 'prompt' is the column for input text\n",
    "                    \n",
    "                    # Append values to respective lists\n",
    "                    ppl_s_values.append(ppl_s)\n",
    "                    ppl_lower_values.append(ppl_lower)\n",
    "                    zlib_values.append(zlib)\n",
    "                    ppl_xl_values.append(ppl_xl)\n",
    "                    \n",
    "                    # Calculate and append pre-zlib perplexity if text is present\n",
    "                    if text:\n",
    "                        pre_zlib_values.append(calculate_pre_zlib(text))\n",
    "                    \n",
    "                    num_samples += 1\n",
    "\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping row with invalid data: {row}\")\n",
    "                    continue\n",
    "\n",
    "        # Calculate average and standard deviation\n",
    "        def calculate_statistics(values):\n",
    "            if values:\n",
    "                average = sum(values) / len(values)\n",
    "                std_dev = statistics.stdev(values) if len(values) > 1 else 0\n",
    "            else:\n",
    "                average = std_dev = 0\n",
    "            return average, std_dev\n",
    "\n",
    "        avg_ppl_s, std_dev_ppl_s = calculate_statistics(ppl_s_values)\n",
    "        avg_ppl_lower, std_dev_ppl_lower = calculate_statistics(ppl_lower_values)\n",
    "        avg_zlib, std_dev_zlib = calculate_statistics(zlib_values)\n",
    "        avg_ppl_xl, std_dev_ppl_xl = calculate_statistics(ppl_xl_values)\n",
    "        avg_pre_zlib, std_dev_pre_zlib = calculate_statistics(pre_zlib_values)  # Calculate pre-zlib stats\n",
    "\n",
    "        return (folder_name, file_name, model_type, model, avg_ppl_s, std_dev_ppl_s, avg_ppl_lower,\n",
    "                std_dev_ppl_lower, avg_zlib, std_dev_zlib, avg_ppl_xl, std_dev_ppl_xl, avg_pre_zlib, std_dev_pre_zlib, num_samples)\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Skipping file with null bytes: {input_file}\")\n",
    "        return (os.path.basename(input_file), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{input_file}' not found.\")\n",
    "        return (os.path.basename(input_file), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file '{input_file}': {e}\")\n",
    "        return (os.path.basename(input_file), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "\n",
    "def summarize_csvs(folder_path, output_file):\n",
    "    \"\"\"\n",
    "    Summarize all CSV files in the specified folder and its subfolders.\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '**', '*.csv'), recursive=True)\n",
    "\n",
    "    # Open output file in append mode\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as summary_file:\n",
    "        writer = csv.writer(summary_file)\n",
    "\n",
    "        # Check if the output file is empty to write header\n",
    "        if os.stat(output_file).st_size == 0:\n",
    "            writer.writerow(['Folder', 'File', 'Model Type', 'Model', 'Avg PPL_S', 'Std Dev PPL_S', 'Avg PPL_Lower', \n",
    "                             'Std Dev PPL_Lower', 'Avg Zlib', 'Std Dev Zlib', 'Avg PPL_XL', 'Std Dev PPL_XL', 'Avg Pre-Zlib', 'Std Dev Pre-Zlib', 'Number of Samples'])\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            result = process_csv(csv_file)\n",
    "            writer.writerow(result)\n",
    "\n",
    "    print(f\"Summary of all CSV files saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'examined'\n",
    "output_file = 'plex_results.csv'\n",
    "summarize_csvs(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine the results to single csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths for the CSV files\n",
    "csv_file1 = 'memorisationfile.csv'\n",
    "csv_file2 = 'plex_results.csv'\n",
    "combined_csv_file = 'combined.csv'\n",
    "\n",
    "# Load the CSV files into pandas DataFrames\n",
    "df1 = pd.read_csv(csv_file1)\n",
    "df2 = pd.read_csv(csv_file2)\n",
    "\n",
    "df1 = df1.rename(columns={'subfolder': 'Folder', 'filename': 'File'})\n",
    "\n",
    "\n",
    "# Merge the DataFrames on the \"Folder\" and \"File\" columns\n",
    "combined_df = pd.merge(df1, df2, on=['Folder', 'File'], how='outer')\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(combined_csv_file, index=False)\n",
    "\n",
    "print(f\"Combined CSV file saved to {combined_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file = combined_csv_file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Specify the columns to move\n",
    "columns_to_move = ['exact', 'fuzzy']\n",
    "\n",
    "# Get the current column order\n",
    "current_columns = df.columns.tolist()\n",
    "\n",
    "# Remove the columns to move from the current order\n",
    "remaining_columns = [col for col in current_columns if col not in columns_to_move]\n",
    "\n",
    "# Append the columns to move to the end of the remaining columns\n",
    "new_order = remaining_columns + columns_to_move\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df = df[new_order]\n",
    "\n",
    "# Save the DataFrame with the new column order to a new CSV file\n",
    "df.to_csv(combined_csv_file, index=False)\n",
    "\n",
    "print(\"Columns reordered and saved to new CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
